# Awesome-Transferability-in-Deep-Learning
 
## Foundation Models

## Supervised Pre-Training

### Improving Standard Pre-Training

* Big Transfer (BiT): General Visual Representation Learning

#### Dataset
#### Architecture
#### Augmentation

### Domain Invariant Learning

####Domain Invariant Representation Learning

Adversarial Training
* Domain generalization with adversarial feature learning
* Dlow: Domain flow for adaptation and generalization
* Deep domain generalization via conditional invariant adversarial networks
* Correlation-aware adversarial domain adaptation and generalization

Theoretical Guarantees
* Learn to expect the unexpected: Probably approximately correct domain generalization
* Domain adversarial neural networks for domain generalization: When it works and how to improve

Moment Matching
* Domain generalization via entropy regularization
* Deep domain confusion: Maximizing for domain invariance
* Visual domain adaptation with manifold embedded distribution alignment
* Domain adaptation via transfer component analysis
* Transfer learning with dynamic distribution adaptation
* Unified deep supervised domain adaptation and generalization
* Domain generalization with optimal transport and metric learning
* Deep coral: Correlation alignment for deep domain adaptation
* Return of frustratingly easy domain adaptation
* Synthetic to real adaptation with generative correlation alignment networks

#### Statistics Normalization

* Two at once: Enhancing
learning and generalization capacities via ibn-net
* Arbitrary style transfer in real-time with adaptive instance normalization
* Batch-instance normalization for adaptively style-invariant neural networks
* Style normalization and restitution for domain generalization and adaptation


### Distributionally Robust Optimization

#### f-Divergence Constraints
* Learning models with uniform performance via distributionally robust optimization
* Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization
* Stochastic gradient methods for distributionally robust optimization with f-divergences

#### Wasserstein Distance Constraints
* Certifying some distributional robustness with principled adversarial training
* Stable adversarial learning under distributional shifts
* Data-driven distributionally robust optimization using the wasserstein metric: Performance guarantees and tractable reformulations
* Generalizing to unseen domains via adversarial data augmentation

### Causal Learning

#### Causal Inference
* Causal inference by using invariant prediction: identification and confidence intervals
* Invariant Causal Prediction for Sequential Data
* Anchor regression: heterogeneous data meets causality
* Invariant Causal Prediction for Nonlinear Models
* Active Invariant Causal Prediction: Experiment Selection through Stability
* Regularizing towards Causal Invariance: Linear Models with Proxies

#### Invariant Risk Minimization
* Invariant risk minimization
* Invariant risk minimization games
* Domain extrapolation via regret minimization
* Out-of-distribution generalization via risk extrapolation (rex)
* Risk variance penalization: From distributional robustness to causality
* Domain generalization using causal matching
* The risks of invariant risk minimization
* Does invariant risk minimization capture invariance?
* Empirical or invariant risk minimization? a sample complexity perspective


### Meta-Learning

#### Update-Based Meta-Learning
* Learning to learn by gradient descent by gradient descent

#### Gradient-Based Meta-Learning
* Model-agnostic meta-learning for fast adaptation of deep networks

#### Memory-Based Meta-Learning
* Meta-learning with memory-augmented neural networks

#### Metric-Based Meta-Learning
* Matching networks for one shot learning

## Unsupervised Pre-Training

### Generative Unsupervised Learning

#### Auto-Regressive Model
* Xlnet: Generalized autoregressive pretraining for language understanding
* Improving language understanding by generative pre-training
* Language models are unsupervised multitask learners
* Pixel recurrent neural networks
* Conditional image generation with pixelcnn decoders
* Wavenet: A generative model for raw audio
* Graphrnn: Generating realistic graphs with deep auto-regressive models
* Generating realistic molecular graphs with optimized propertie.
* Graph convolutional policy network for goal-directed molecular graph generation

#### Auto-Encoding Model
Context Prediction Model
* Distributed representations of words and phrases and their compositionality
* Enriching word vectors with subword information
* Deepwalk: Online learning of social representations
* Line: Large-scale information network embedding

Denoising Auto-Encoding Model
* Bert: Pretraining of deep bidirectional transformers for language understanding
* Spanbert: Improving pre-training by representing and predicting spans
* Ernie: Enhanced representation through knowledge integration
* Ernie: Enhanced language representation with informative entities
* Gpt-gnn: Generative pre-training of graph neural networks

Variational Auto-Encoding Model
* Auto-encoding variational bayes
* Neural discrete representation learning
* Generating diverse high-fidelity images with vq-vae-2
* Variational graph auto-encoders
* Deep variational network embedding in wasserstein space
graph: A generative model for joint community detection and node representation learning

### Contrastive Unsupervised Learning
* Noise-contrastive estimation: A new estimation principle for unnormalized statistical models

#### Context-Instance Contrast
Predict Relative Position
* Self-supervised visual feature learning with deep neural networks: A survey
* Unsupervised visual representation learning by context prediction
* Learning image representations by completing damaged jigsaw puzzles
* Unsupervised learning of visual representations by solving jigsaw puzzles
* Iterative reorganization with weak spatial constraints: Solving arbitrary jigsaw puzzles for unsupervised representation learning
* Unsupervised representation learning by predicting image rotations
* Self-supervised learning of pretext-invariant representations
* Bert: Pretraining of deep bidirectional transformers for language understanding
* Albert: A lite bert for self-supervised learning of language representations

Maximize Mutual Information
* Learning deep representations by mutual information estimation and maximization
* Representation learning with contrastive predictive coding
* Learning representations by maximizing mutual information across views
* Contrastive multiview coding
* A mutual information maximization perspective of language representation learning
* Deep graph infomax
* Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization
* Contrastive multi-view representation learning on graphs
* Strategies for pre-training graph neural networks
* Self- supervised graph representation learning via global context prediction

#### Instance-Instance Contrast
Cluster Discrimination
* Deep clustering for unsupervised learning of visual features
* Local aggregation for unsupervised learning of visual embeddings
* Clusterfit: Improving generalization of visual representations
* Unsupervised learning of visual features by contrasting cluster assignments
* Self-supervised pretraining of visual features in the wild
* Multi-stage self-supervised learning for graph convolutional networks
* When does self- supervision help graph convolutional networks?

Instance Discrimination
* Unsupervised feature learning via non-parametric instance discrimination
* Contrastive multiview coding
* Momentum contrast for unsupervised visual representation learning
* Self-supervised learning of pretext-invariant representations
* A simple framework for contrastive learning of visual representations
* Improved baselines with momentum contrastive learning
* What makes for good views for contrastive learning
* Bootstrap your own latent: A new approach to self-supervised learning
* Exploring simple siamese representation learning
* Representation learning via invariant causal mechanisms
* Gcc: Graph contrastive coding for graph neural network pre-training


## Model Adaptation

### Fine-Tuning

### Knowledge Distillation

### Prompt Learning

## Domain Adaptation




